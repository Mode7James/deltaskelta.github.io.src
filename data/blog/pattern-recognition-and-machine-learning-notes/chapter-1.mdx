---
title: "Pattern Recognition and Machine Learning: Chapter 1 Notes"
published: false
createdAt: 2019-05-07T00:59:25.732Z
updatedAt: 2019-06-05T01:33:50.557Z
categories:
  - Pattern Recognition
  - Machine Learning
  - Chapter 1
images:
  - ./probability-theory-3x5-grid.png
---

import { M } from '../../../src/components/math';

- **supervised learning** has both input and target vectors that have been labeled
- **clasification** assigns each input vector to a discrete set of output categories
- **regression** outputs one or more continuous variables (such as predicting factory output or housing prices)
- **clustering** finds groups of related clusters or distributions of data, given input data
- ** reinforcement learning** finds suitable actions in a given situation to maximize a reward

The hypothesis function below is a non-linear function of `x` but a linear function of the `w` coefficients

<M b="y(x, w) = w_0 + w_1 x + w_2 x^2 + ... + w_m x^m = \sum_{j=0}^m w_j x^j" />

**linear learnable parameters** have important properties called linear models which will be covered in chapter 3 and 4

The sum of squared errors function is a quadratic function of the w coefficients <M i="\implies"/> that the derivative
with respect to coefficients will be linear.

<M b="E(w) = \frac{1}{2} \sum_{n=1}^N \{y(x_n, w) - t_n\}^2" />

> **NOTE** during optimization, the differentiation occurs for every weight in the weight matrix so that even though
> this appears to be a quadratic function, it is composed from a posibly higher dimensional function (the hypothesis) so
> therefore it is not guaranteed that a global minimum can be found.

Choosing to low of an order of polynomial for the hypothesis function will make it likely that there is an underfit to
the data because there will not be enough freedom for the hypothesis bend to the data. On the other hand, choosing a
polynomial of the hypothesis that is too high will likely cause overfitting if there is not enough training data to
ensure that the curve doesn't swing wildly in order to minimize the error function.

When using different sizes of data sets using the above functions, it may help to compare the error along the same scale
by using the root mean squared error function. This ensures that the error is on the same scale and in the same units as
the target varaible <M i="t" />.

 <M b="E_{RMS} = \sqrt{2E(w)/N}" />

Lambda regularization can be used to combat overfitting by forcing the weights towards 0 by imposing a penalty based in
the magnitude of the weight vector.

 <M b="\tilde{E}(w) = \frac{1}{2} \sum_{n=1}^N \{ y(x_n, w) - t_n \}^2 + \frac{\lambda}{2}||w||^2" />

In statistics, this is known as **shrinkage**, a quadratic regularizer is called **ridge regression**, and when dealing
with neural networks, it can be called **weight decay**.

Multiple values need to be tried in order to arrive at the best <M i="\lambda" /> value because too low of a value can
result in overfitting, while too high of a vlaue can result in underfitting. If there is enough training data to warrant
having different data sets, then plotting the differences in error between the data sets for different lambda values
could show where the best value lies (which is where the gap is the smallest).

## Probability Theory

<props.imgs.Img1 width="30%" align="left" />

The example given in the book is a 3x5 grid pictured here. Something important to note about probabilities from the grid
is that they are not independent from one another, a pick from `x` implies that there is a pick from `y` as well which
means that it is not possible to pick an `x` without picking a `y`.

since picking an `x` value also implies that some `y` value is chosen, it can be said that the probability of <M i="x_i" />
can be found by summing away the `y` probabilities. This is the **sum** rule or the **marginal** probability.

<M b="p(x_i, y_i) = \frac{n_{ij}}{N} = \frac{1}{15}" />
<M b="c_i = \sum_{j} n_{ij} = \frac{1}{15} + \frac{1}{15} + \frac{1}{15}  = \frac{3}{15}" />
<M b="p(x_i) = \frac{c_i}{N} = \frac{3}{15}"/>

<M b="p(x_i) = \sum_{j=1}^L p(x_i, y_j)" />

The **product rule** states that the joint probability (probability of `x` and `y`) can be found by the following
relationship between the conditional probability and the prior probability.

<M b="p(y_j | x_i) = \frac{n_{ij}}{c_i}" />
<M b="p(x_i, y_j) = \frac{n_{ij}}{N} = \frac{n_{ij}}{c_i} * \frac{c_i}{N} = p(y_j | x_i) p(x_i)" />

When the joint probability is being considered, the order doesn't matter, so <M i="p(x_i, y_j) = p(y_j, x_i)" />, and
therefore we can do the following.

<M b="p(x_i | y_j) = \frac{p(y_j | x_i) p(x_i)}{p(y_j)}" />

This is true because <M i="p(x_i | y_j)p(y_j) = p(y_j | x_i)p(x_i)" />, and the denominator can even be rewritten in
terms of the numerator because of the **sum** rule. Page 15-17 of the book go into a detailed example using boxes of
fruit.

<M b="p(x_i | y_j) = \frac{p(y_j | x_i) p(x_i)}{\sum_{i=1}^L p(y_j | x_i)p(x_i)}" />

## Probability Densities

The probability that `x` will fall in some interval of a probability density function is given by the area under the
curve in that interval. In order for this to be true, the density must integrate to 1 and be <M i="\ge 0" />.

<M b="p(x \in (a, b)) = \int_a^b dx \, p(x)" />

> TODO: work on understanding this part better

Under a non-linear trasnformation of variable, a probability density function trasnforms differently from a simple
function due to the Jacobian factor.

I think that under a non-linear transformation of the probability distribution, the probabilities at each point must be
the same, therefore the mode cannot be guaranteed to be the same because if the distribution changes from a normal
distribution to a skewed distribution with the same probabilities at the trasnformed points of the curve, then the new
curve will have to conform differently than a function.

### Probability Densities With Bayes Theorem

The sum and product rules of probability as well as Bayes Theorem apply equally to the case of probability distributions

<M b="
  p(x) = \int p(x, y) \delta y \\
  p(x, y) = p(y|x) p(x)
" />

### Expectationsa of Functions

to get the expected value of a variable with a probability distribution, we can take the following...

<M b="\mathbb{E}_x = \int x \; p(x) \; \delta x" />

The average value of <M i="f(x)" /> under a probability distribution <M i="p(x)" /> is called the expectation
of <M i="f(x)" /> , and is denoted by <M i="\mathbb{E}[f]" />. It is important to note that this is the expectation of
the probability density function and not of the variable. For example if the probability density function is a normal
distribution for human height, we would be getting the expected probability and not the expected height. For
discrete and continuous distributions respectively...

<M b="
  \mathbb{E}[f] = \sum_x p(x)f(x) \\
  \mathbb{E}[f] = \int p(x)f(x) \delta x
" />

If we were instead to calculate an approximation of the expectation, it can be given by a simple average of points irrespective
of their probability. This becomes exact as <M i="N \rightarrow \infty" /> because more probable events will occur more
often and therefore the average of all <M i="f(x)" /> will become closer and closer to exact.

<M b="\mathbb{E}[f] \approx \frac{1}{N} \sum_{n=1}^N f(x_n)" />

> **TODO** how to write the right side of the equation below? (page 20)

<M i="\mathbb{E}[f(x, y)]" /> is the average of the function <M i="f(x, y)" /> with respect to <M i="x" />,
and can be calculated by

<M b="\mathbb{E}_x[f(x, y)] = p(x) f(x, y)" />

for conditional distributions, it can be said that the conditional expectation with respect to a conditional
distribution is given by...

<M b="\mathbb{E}_x[f|y] = \sum_x p(x|y)f(x)" />

the variance of <M i="f" /> is given by <M i="var[f] = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2]" />.
