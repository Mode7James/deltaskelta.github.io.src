---
title: "Making a Plain Text Corpus of Wikipedia"
published: true
createdAt: Thu, 24 Mar 2016 01:13:55 GMT
updatedAt: Sat, 19 Jan 2019 11:53:50 GMT
categories:
  - Programming
  - Projects
  - Python
---

I needed to get a plaintext corpus of wikipedia for a website I have been working on (langalang.com) I did some
preliminary research online and came across this method.... [https://blog.afterthedeadline.com/2009/12/04/generating-a-plain-text-corpus-from-wikipedia/](https://blog.afterthedeadline.com/2009/12/04/generating-a-plain-text-corpus-from-wikipedia/)

It seemed like a good approach but something always went wrong during step 4. There were so many processes hanging
that almost nothing was getting done. I may be wrong, but I didn't know how to investigate further since the code
that runs it is quite complex and written with a lot of java and php, two languages I am not so familiar with. On top
of this, my machine at home was pretty weak and I had to rent expensive servers from Digital Ocean which were costing
me money by the hour.

I thought about trying to do it myself, but I thought there had to be people who did this before and already sifted
through the Wikimedia xml to eliminate unnecessary text.

I decided to have a look at other methods and I came across one that worked a whole lot better for me.

## Gensim

I found the tool here. [https://radimrehurek.com/gensim/corpora/wikicorpus.html](https://radimrehurek.com/gensim/corpora/wikicorpus.html)
This is an extremely powerful tool that can do so much more than I need to. I could have written my own script to use
the methods contained in this file, but I found another module that does almost exactly what I need so I decided to
use that. [https://github.com/piskvorky/sim-shootout](https://github.com/piskvorky/sim-shootout)

## Step 1: Installing and downloading everything

```bash
#Making a direcotry to work in and changing into it
mkdir wikicorpus
cd wikicorpus
#Creating and acitvating a python virtualenvironment
virtualenv venv
source venv/bin/activate
#Installing the gensim module into the virtual environment
pip install gensim
#Downloading the sim-shootout module (it uses gensim itself to make the corpus)
wget https://github.com/piskvorky/sim-shootout/archive/master.zip
unzip master.zip
```

## Step 2: Getting What I Need From the `sim-shootout` Methods

This is where you may start to differ. I was after a plain text corpus of all the articles from wikipedia, with all
of the markup and non-essential text.

The `prepare_shootout.py` file does what I wanted and so much more. In fact I commented out a ton of code from the
main loop to only isolate what I needed....

```python
if __name__ == '__main__':
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info("running %s" % ' '.join(sys.argv))

    # check and process input arguments
    program = os.path.basename(sys.argv[0])
    if len(sys.argv) < 3:
        print globals()['__doc__'] % locals()
        sys.exit(1)
    infile, outdir = sys.argv[1:3]
    outfile = lambda fname: os.path.join(outdir, fname)

    # extract plain text from the XML dump
    preprocessed_file = outfile('title_tokens.txt.gz')
    if not os.path.exists(preprocessed_file):
        id2title = []
        with gensim.utils.smart_open(preprocessed_file, 'wb') as fout:
            for docno, (title, tokens) in enumerate(convert_wiki(infile, PROCESSES)):
                id2title.append(title)
                try:
                    line = "%s\t%s" % (title, ' '.join(tokens))
                    fout.write("%s\n" % gensim.utils.to_utf8(line)) # make sure we're storing proper utf8
                except:
                    logger.info("invalid line at title %s" % title)

        #I commented out all of this code because I wasn't after any of this stuff, above is the plain text file only...
        '''gensim.utils.pickle(id2title, outfile('id2title'))

    # build/load a mapping between tokens (strings) and tokens ids (integers)
    dict_file = outfile('dictionary')
    if os.path.exists(dict_file):
        corpus = ShootoutCorpus()
        corpus.input = gensim.utils.smart_open(preprocessed_file)
        corpus.dictionary = gensim.corpora.Dictionary.load(dict_file)
    else:
        corpus = ShootoutCorpus(gensim.utils.smart_open(preprocessed_file))
        corpus.dictionary.filter_extremes(no_below=20, no_above=0.1, keep_n=50000)  # remove too rare/too common words
        corpus.dictionary.save(dict_file)
        corpus.dictionary.save_as_text(dict_file + '.txt')

    # build/load TF-IDF model
    tfidf_file = outfile('tfidf.model')
    if os.path.exists(tfidf_file):
        tfidf = gensim.models.TfidfModel.load(tfidf_file)
    else:
        tfidf = gensim.models.TfidfModel(corpus)
        tfidf.save(tfidf_file)

    # build/load LSI model, on top of the TF-IDF model
    lsi_file = outfile('lsi.model')
    if os.path.exists(lsi_file):
        lsi = gensim.models.LsiModel.load(lsi_file)
    else:
        lsi = gensim.models.LsiModel(tfidf[corpus], id2word=corpus.dictionary, num_topics=NUM_TOPICS, chunksize=10000)
        lsi.save(lsi_file)

    # convert all articles to latent semantic space, store the result as a MatrixMarket file
    # normalize all vectors to unit length, to simulate cossim in libraries that only support euclidean distance
    vectors_file = os.path.join(outdir, 'lsi_vectors.mm')
    gensim.corpora.MmCorpus.serialize(vectors_file, (gensim.matutils.unitvec(vec) for vec in lsi[tfidf[corpus]]))

    logger.info("finished running %s" % program)'''
```

The active code above will create a `title_tokens.txt.gz` file which will consist of the article title, followed by
the complete article text, all on one line.

## Step 3: Changing the `Gensim` module to fit my needs

The Gensim module takes some liberties in filtering out words that are less than two characters in length. This is no
good for me becuase I want a complete English corpus, not filtering out the one letter words 'A' and 'I.' to do this
I had to go int `gemsim.utils.py` and edit the `simple_preprocess` method.

```python
#I changed the min_len to 1 to include the words 'I' and 'A'
def simple_preprocess(doc, deacc=False, min_len=1, max_len=15):
    """
    Convert a document into a list of tokens.

    This lowercases, tokenizes, de-accents (optional). -- the output are final
    tokens = unicode strings, that won't be processed any further.

    """
    tokens = [
        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')
        if min_len <= len(token) <= max_len and not token.startswith('_')
    ]
    return tokens
```

This has another side effect of adding random non-english words to the output of the article text. Things like 'i.e.'
get split into 'i e' when punctuation is removed and then they are counted as words in the article. In the next
method which goes over every article text, counting the occurrences of each word, I eliminate these unwanted one
letter words. (There are only two one-letter words in English, I could have done that in this step which would have
been more efficient but it was easy enough and quick to do it separately.)

The Gensim module also does not count wikipedia articles with less than 50 words, I decided to keep that constraint
but it could also easily be modified.

## Step 4: Going Over The Full Articles and Counting Word Occurrences

I wrote this script to iterate over each word in the plaintext output file and count each occurrence, add them
together and give a final list of each word and its number of occurrences. One per line

```python
import sys
import operator

#Open the input file which is a text file with the article title and text all on one line
infile = open(sys.argv[1])

tokens = {}
for line in infile:
  line = line.split()
  for word in line:
    if word in tokens:
      tokens[word] += 1
    else:
      tokens[word] = 1
infile.close()

tokens = sorted(tokens.items(), key=operator.itemgetter(1), reverse=True)

#argv[2] should be a plain string without .txt
out_title = sys.argv[2] + '.txt'
outfile = open(out_title, 'w+')
for tup in tokens:
  #This is where I skip words that are not 'I' and 'A'
  if len(tup[0]) == 1 and (tup[0] != 'i' and tup[0] != 'a'):
    pass
  else:
    outfile.write('%s\t%s\n' % (tup[0], tup[1]))
outfile.close()
```

## Step 5: Preparing the Corpus

At this point I am ready to prepare the corpus, I need to run `prepare-shootout.py` which will process the database
dump from wikipedia into a compressed plain-text file. In order to get the database dump you should go to
[https://dumps.wikimedia.org/backup-index.html](https://dumps.wikimedia.org/backup-index.html) and select the wiki
you would like to download. From there you need the file that is labeled 'somewiki-somedate-pages-articles.xml.bz2'...

```bash
#Downloading the wiki dump with wget
wget https://dumps.wikimedia.org/enwiki/20160305/enwiki-20160305-pages-articles.xml.bz2
```

This is the name of the file that I downloaded, wikimedia makes database dumps every so often so the current file
will change all the time. That is a ~12GB download. After it finishes, you are ready to process it...

```bash
#Processing the database dump with prepare-shootout.py
mkdir output
python sim-shootout-master/prepare-shootout.py enwiki-20160305-pages-articles.xml.bz2 output/
```

This will take a long time to run depending on how powerful your machine is. I am using a t2.medium Amazon EC2
instance, and it has been running for about 12 hours so far...

I ended up stopping that instance and starting up the smallest of the compute optimized instance which went much
faster. (Around 3-4 hours I believe.)

After running this you should have a file names title_tokens.txt.gz. If you unzip this file it will have the article
title and the article text with all the unnecessary markup and headings removed. YOu can then use this file to
process it however you want.
