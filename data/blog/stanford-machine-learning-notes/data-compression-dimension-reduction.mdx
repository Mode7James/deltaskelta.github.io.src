---
title: Data Compression and Dimension Reduction
published: true
createdAt: 2019-03-19T14:00:27.341Z
updatedAt: 2019-03-20T03:22:08.447Z
categories:
  - Programming
  - Algorithms
  - Machine Learning
  - Neural Networks
---

It can be useful to compress feature data for the reasons of saving space, memory, and compute power when
running machine learning algorithms. Sometimes there may be obvious features that can be combined such as length in cm
and feet, and sometimes it might be more obscured. It is also useful to compress data into a viewable set of dimensions,
such as 2 or 3 so that visualizations can be made.

## Principal Component Analysis

As an example, in order to project a 2 dimensional plot into one dimension, one could find a line that minimizes the
square error and use that line as the new 1 dimensional representation of the 2 dimenstional data. This is different
from linear regression because in linear regression, y is the variable that is trying to be predicted. Therefore the
square errors are measured vertically from the line to the data. In PCA, the differences would no be measured with
respect no any special variable.

Before applying PCA it is important to preprocess the data with feature scaling and mean normalization. In the
following formula, `s_j` is some calculation regarding the range of values of x. Standard deviation would be
the most likely scenario because that would reduce each feature to the amount that it is above or below the mean.

<BlockMath math="
  \begin{aligned}
    &\mu_j = \frac{1}{m} \sum_{i=1}^m x^{(i)}_j \\
    &\text{then replace each } x_j^{(i)} \text{ with } \frac{x_j - \mu_j}{s_j}
  \end{aligned}
"/>

1. Compute covariance matrix <InlineMath math="\Sigma = \frac{1}{m} \sum_{i=1}^n (x^{(i)})(x^{(i)})^T" />. This is done
by multiplying the feature vector by itself transposed, and divided the the constant `m`.
2. Compute the eigenvectors of the matrix <InlineMath math="\Sigma"/>. In Octave this can be accomplished by
[U, S, V] = svd(Sigma) and then take the first k columns of the `U` variable. k is the new number of dimensions that are
desired. This U matrix will be `n x n`.
3. Multiply the reduced U by a training example. <InlineMath math="z = U_{reduced}^T * x^{(i)}" />. In this case `U` is an
`n x n` matrix. `Ureduced` is an `n x k` matrix. The ith training example is an `n x 1` matrix. The result of the
multiplication will be a `k x 1` matrix.

If we wanted to go the other way and get an approximate value of x back out, we can do the
following <InlineMath math="X_{approx} = U_{reduce} * z" />. `Ureduced` is an `n x k` and z is a `k x 1`, so the result
will be a `n x 1` or the original length of the feature set. If the dimensionality was changed when choosing `Ureduced`,
then the dimensionality of the resulting `Xapprox` will indeed be approximate, but if the dimensionality was not
changed, then it would be the exact value. This is because when the number of dimensions are changed, approximations
based on the lowest mean square error are made which cannot be reversed. There is no practical reason that the number of
dimensions would not be changed, this statement was just for a theoretical example.

## Choosing The Proper Number of Dimensions (k value)

<BlockMath math="
  \frac
    {\frac{1}{m} \sum_{i=1}^m \begin{Vmatrix} x^{(i)} - x^{(i)}_{approx} \end{Vmatrix}^2}
    {\frac{1}{m} \sum_{(i=1)}^m \begin{Vmatrix} x^{(i)} \end{Vmatrix}^2}
    \leq 0.01
"/>

- The numerator of the above equation is the average of the sum of the squared distance between the original x and the
`x_approx`, in other words, how much on average did x change by.
- The denominator is the average of the squared distances from the origin to x.

When we are changing the dimensionality of the training data, we want to reduce the dimensions with as little mutation
to the data as possible, therefore a good measure would be to keep the value of this equation low such as the above
mentioned 1%. In other words, 99% of the variance is retained.

This can be done by looking at the return values of the `svd()` function call. The diagonal of the `S` matrix can be
used to compute the following. `1 - ...` gives the amount of change in the data caused by compression. Without the `1 -
...`, the fraction would give the amount of retained variance (or integrity?) of the compressed data.

<BlockMath math="1 - \frac{\sum_{i=1}^k S_{i,i}}{\sum_{(i=1)^n S_{i,i}}}" />

## Notes

- PCA should never be used to address overfitting. There are other ways such as regularization that does this more
  reliably.
- Before deciding to use PCA, be sure that it is actually necessary by trying without PCA and only using it when a need
  has been demonstrated.
