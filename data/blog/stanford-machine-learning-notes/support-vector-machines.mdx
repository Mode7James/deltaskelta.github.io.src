---
title: Support Vector Machines
published: true
createdAt: 2019-03-18T12:20:32.908Z
images:
  - ./support-vector-machine-large-margin.png
  - ./negative-log-sigmoid.png
  - ./negative-log-one-minus-sigmoid.png
updatedAt: 2019-04-20T11:39:20.418Z
categories:
  - Programming
  - Algorithms
  - Machine Learning
  - Neural Networks
  - Week 7
---

import { M } from '../../../src/components/math';

<props.imgs.Img1 width="25%" align="left" />

Support Vector Machines are also known as large margin classifiers because they increase the margin of the decision
boundary. In the image below, the dark green line would be the line that the SVM would make for the decision boundary
because it provides the largest margin between the to sets of data. The other lines are valid, but they don't seem to be
the natural way to draw the boundary between the two sets.

<props.imgs.Img2 width="30%" align="right" /> In order to achieve this, the cost function needs to be changed to not include the lambda term. It instead includes a
`C` regularization term which is multiplied by the cost sum. The <M i="h_\theta(x)" /> function is also
changed from being the log of the sigmoid to being something different as is shown in the Octave charts here. The light
blue lines are the log of the sigmoid and the dark green lines are the new cost function.

What this does is basically change the cost from always approaching 0 to actually
being 0 if <M i="\theta^TX"/> is greater than 1 or less than 1
(for the appropriate term of the cost function <M i="cost_1 \gg 0 \text{ and } cost_0 \ll 0" />).

<props.imgs.Img3 width="30%" align="right" />

- If the C term is very large, SVM is more likely to overfit the data and draw an unintuitive decision boundary.
- If the C term is very small, it is more likely to underfit the data.

<M b="
  \theta^{min} C \sum_{i=1}^m \lbrack y^{(i)} cost_1(\theta^T x^{(i)}) + (1 - y) cost_0(\theta^Tx^{(i)}) \rbrack +
  \frac{1}{2} \sum_{i=1}^n \theta^2_j
" />

## Kernels

Kernels take in a feature and give an output based on its proximity to landmarks in the features. The landmarks
themselves can be other features. The formula for finding the similarity is as follows...

<M b="f_1 = similarity(x,l^{(1)}) = exp \Big( - \frac{|| x - l^{(1)}||^2}{2\sigma^2} \Big)" />

This makes the value of <M i="f_1" /> nearly equal to 1 if the feature x is close to the landmark. If the
feature x is far from the landmark, the value oof <M i="f_1" /> is close to 0.

- With a large <M i="\sigma" /> value, it is more likely to underfit the data (high bias, low variance, and
  a gradual slope on the 3d graph of the distance)
- With a small <M i="\sigma" /> value, it is more likely to overfit the data (lower bias, high variacne and a
  steeper slope of the 3d graph of the distance)

## Using an SVM

When using an SVM we need to specify a choice of parameter C, and a choice of a kernel (similarity function). Some of
the options are as follows.

1. No kernel (linear kernel). predicts <M i="y = 1" /> when <M i="\theta^TX >= 0" />. This is good when there are a
large number of features and a small number of training examples. Or when the decision boundary between the features is
linear in nature.
2. Gaussian kernel (the similarity function above). Good for when the number of features is small and the number of
examples is large, with a non linear decision boundary.

Feature scaling should be applied before using the Gaussian kernel.

- if n (number of features) is large relative to m, use logistic regression or an SVM with linear kernel.
- if n is small relative to to m and m is intermediate, use an SVM with a Gaussian kernel.
- if n is small and m is very large, use logistic regression or SVM without a kernel.
- neural networks also work for most cases, but they may be slow to train.
