---
title: Multivariate Regression Analysis
published: true
createdAt: 2019-02-06T15:07:03.555Z
updatedAt: 2019-07-21T14:58:16.780Z
categories:
  - Machine Learning
  - Algorithms
  - Stanford
---

every feature in a machine learning model should have a scale of approximately -1 to 1 in order to make the gradient
descent an easier process. As long as it is something small and consistent.

Another way to go about it is to normalize the mean to 0 and divide by the range of values. the `mu` is the average of
all the values. In the example given, the average house age was 38 years old and the range of values were from 30 to 50
years. This means that for the average case, the value would be 0

$$
\frac{x_i - \mu_i}{s_i}
$$

The gradient descent for each theta value is as follows:

$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$

When running a regression analysis it is important to have the right values for $ \alpha $ because if
it is too big then it will cause a "wobble" by overshooting the minimum and instead of converging on the minimum value,
it will actually diverge.

## Polynomial Regressions

if the data with respect to a feature appears to be quadratic or cubic, the hypothesis function should show this. This
can be done by taking the same features and squaring or cubing them to fit the data shape. In the following
example, $ \theta_1x_1 $ and $ \theta_2 x_2^2 $ are the same metric split into
different features such as `size` and `size * size`

$$
\begin{aligned}
  & h_\theta(x) \quad linear \quad = \theta_0 + \theta_1x_1 \\
  & h_\theta(x) \quad quadratic \quad = theta_0 + \theta_1x_1 + \theta_2x_2^2 \\
\end{aligned}
$$

## Normal Equations

The gradient descent method for finding the minimum error of the theta values in the hypothesis function can be solved
by an equation instead of iterating through the algorithm. This is better when the number of features is small because
it can be calculated easily, but when the number of features grows large (like $ 10^6 $) it can become
costly and it would be relatively more efficient to use gradient descent.

$$
\theta = (X^TX)^{-1}X^Ty
$$

#### Gradient Descent

- needs an alpha value (a multiple to step by the derivative)
- need many iterations
- $ O(kn^2) $
- works better when n is large

#### Normal Equations

- no need for alpha
- no iterations
- $ O(n^3) $
- slow if n is large
