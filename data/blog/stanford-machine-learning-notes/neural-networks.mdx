---
title: Neural Networks
published: true
createdAt: 2019-02-26T14:20:26.310Z
updatedAt: 2019-07-21T15:08:47.860Z
categories:
  - Programming
  - Algorithms
  - Machine Learning
  - Neural Networks
  - Stanford
---

Neural networks solve the problem in a different way than plain linear or logistic regression. With plain linear or
logistic regression, a function must be chosen that models the structure of the data. By having multiple neurons in
hidden layers, each neuron can make its own function that when combined with other neurons, can simulate more complex
functions. For example the function of a circle can be simulated by taking multiple intersecting lines and curves
to mark out a section on a graph that looks like a circle.

Each feature (column) gets fed into each neuron (row) in the first hidden theta layer in forward propogation
in the following way.

$$
  \begin{bmatrix}
    x_1^1 \\
    x_1^2 \\
    x_1^3
  \end{bmatrix}
  \to
  \begin{bmatrix}
  ...
  \end{bmatrix}
  \to
  h_\theta(x)
$$

Inside the hidden layer is a matrix of theta values which looks like this

$$
  \begin{aligned}
  &\begin{bmatrix}
    x_1^1 \\
    x_1^2 \\
    x_1^3 \\
  \end{bmatrix}
  \to
  \begin{bmatrix}
    a_1 \\
    a_2 \\
    a_3
  \end{bmatrix}
  \to
  h_\theta(x) \\
  &\begin{bmatrix}
    x_1^1 \\
    x_1^2 \\
    x_1^3 \\
  \end{bmatrix}
  \to
  \begin{bmatrix}
    g(\theta_{10}x_0 + \theta_{11}x_1 + \theta_{12}x_2)) \\
    g(\theta_{20}x_0 + \theta_{21}x_1 + \theta_{22}x_2)) \\
    g(\theta_{30}x_0 + \theta_{31}x_1 + \theta_{32}x_2))
  \end{bmatrix}
  \to
  g(\theta_{10}^{(2)}a_0^{(2)} + \theta_{11}^{(2)}a_1^{(2)} + \theta_{12}^{(2)}a_2^{(2)} + \theta_{13}^{(2)}a_3^{(2)})
  \end{aligned}
$$

The jump from the second layer to the third layer where the hypothesis outputs a prediction is similar to the logistic
regression with no neural network. The difference in the neural network is that each input feature can be mapped to each
dimension of the hidden layer vector and the network can then combine the inputs according to the theta values instead of
manually coming up with a function to fit intended shape of the data.

## The Example Problem

#### The Cost Function

The example problem for this week `ex3.m` first had us implement a vectorized cost function and gradient (partial
derivative). The arguments to this function are as follows.

1. The `X` is the entire training set, which is a `5000x400` matrix of 5000 examples with 400 features each. Each pixel in the
   handwritten digit is a feature.
2. It also takes thetas for each feature plus a bias, so 401 theta values
3. `y` is the expected output of the classification, which is a vector of `0`'s or `1`'s
4. lambda is the lambda regularization value

```matlab
function [J, grad] = lrCostFunction(theta, X, y, lambda)
```

#### The One VS. All Training

The second function that we had to implement was the training of the theta values for the on vs all comparisons.

```matlab
function [all_theta] = oneVsAll(X, y, num_labels, lambda)
```
1. `X` is the `5000x400` training set
2. `y` is the expected values of the digits. This is a `5000x1` matrix with the actual value of the 1-10 digit.
3. `num_labels` are the number of different labels or possible outcomes that we have.
4. `lambda` is the lambda regularization value that is to be fed to the cost function.
5. The return value `all_theta` is a `10x401` matrix. Each row of the matrix is used to identify one of the 10 labels
that we have.

to implement the function I made a loop that calls the `fmincg` function for every label that we have. The result is a
`10x401` matrix of theta values where each of the 10 rows is a set of theta values for every pixel that is trained to
identify whether an input is a single label (digit). If an input was to be tested, it would have to be tested against
all of the rows so that the question "is this a 1?" "is this a 2?" ... for every possible label is asked and answered.

the `y` input to the cost function is done in the form of `y == c` where `c` is the loop iterator which also
corresponds to the digit in the `oneVsAll` `y` arg. The argument that is passed to the cost function will be a `5000x1`
vector of `0`'s and `1`'s where the `1`'s are the one vs all digit we are trying to identify for this particular set of
thetas.

#### The prediction

for the prediction, the function takes two args which are...

1. `all_theta` which is a `10x401` matrix where each row is a set of theta values to make a decision about a single
label.
2. `X` which is a `5000x400` matrix of 5000 training examples with 400 features each.
3. the return value `p` should be a `5000x1` vector of output hypotheses

```matlab
function p = predictOneVsAll(all_theta, X)
```

Multiplying the `all_theta` matrix with the `X'` matrix
gives $ [10 x 401] * [401 x 5000] = [10 x 5000] $ so
in the end there will be a `10x5000` matrix where each column will be an output of something near 0 or near 1. If we
take the index of the max then we can construct the return value.

## Prediction With A Neural Network

the last problem in the set was to make a prediction using a neural network. This is done by multiplying the `5000x401`
input matrix by the `25x401` matrix (transposed) that makes up the first hidden layer. The output from this layer
is $ [5000 x 401] * [401 x 25] = [5000 x 25] $. I like to visualize this as 25 different interim predictions, or a
secondary (and smaller) set of features on the way to the final layer and the actual prediction.

$$
  \begin{aligned}
    & \begin{bmatrix}
      x_{0,0} & ... & x_{0,401} \\
      & . & \\
      & . & \\
      x_{5000,0} & ... & x_{5000,401}
    \end{bmatrix}
    *
    \begin{bmatrix}
      \theta^{(2)}_{0,0} & ... & \theta^{(2)}_{0,25} \\
      & . & \\
      & . & \\
      \theta^{(2)}_{401,0} & ... & \theta^{(2)}_{401,25}
    \end{bmatrix} \\
    = &
    \begin{bmatrix}
     (x_{0,0} * \theta_{0,0} + ... + x_{0, 401} * \theta_{401,0}) & ... & (x_{0,0} * \theta_{0,25} + ... + x_{0,401} * \theta_{401,25}) \\
     & . & \\
     & . & \\
     & . &
    \end{bmatrix}
  \end{aligned}
$$

Then the output from the second layer is ready to be fed into the third layer. The output from the second layer can be
seen as 5000 inputs with 25 features each since the first layer has cut the original 401 input features down to 25.
The $ l2 $ layer has 26 theta values, one for each of the outputs of $ l1 $ plus an
extra bias unit. This gives us the following multiplication of
matrices $ [5000 x 26] * [26 x 10] = [5000 x 10] $.

$$
  \begin{aligned}
    &\begin{bmatrix}
      a^{(2)}_{0,0} & ... & a^{(2)}_{0,25} \\
      & . & \\
      & . & \\
      a^{(2)}_{5000,0} & ... & a^{(2)}_{5000,25}
    \end{bmatrix}
    *
    \begin{bmatrix}
      \theta^{(3)}_{0,0} & ... & \theta^{(3)}_{0,10} \\
      & . & \\
      & . & \\
      \theta^{(3)}_{26,0} & ... & \theta^{(3)}_{26,10}
    \end{bmatrix} \\
    = &
    \begin{bmatrix}
    (a^{(2)}_{0,0} * \theta^{(3)}_{0,0} + ... + a^{(2)}_{0,26} * \theta^{(3)}_{26,0}) & ... & (a^{(2)}_{0,0} *
    \theta^{(3)}_{0,10} + ... + a^{(2)}_{0,25} * \theta^{(3)}_{25,10}) \\
    & . & \\
    & . & \\
    & . &
    \end{bmatrix}
  \end{aligned}
$$

The output from this layer is the one vs all prediction, so each row of the `5000x10` output matrix is a row
of $ \approx 0 \text{ or } \approx 1 $. If index 5 is approximately 1, then the prediction is that
the hadnwritten digit is a `5`.

## Back Propogation

#### The Cost Function

$$
  J(\theta) = - \frac{1}{m} \sum^m_{i=1} \sum^K_{k=1}
  [y^{(i)}_k log((h_\theta(x^{(i)}))_k) + (i - y^{(i)}_k) log(1 - (h_\theta(x^{(i)}))_k)] +
  \frac{\lambda}{2m} \sum^{L-1}_{l=1} \sum^{s_l}_{i=1} \sum^{s_{l+1}}_{j=1} (\theta_{i,j}^{(l)})^{2}
$$

The more complicated cost function comes from the fact that there are multiple output layers `K` in the one vs all
prediction. All of the `K` one vs all output layers must be summed together before summing up the cost for each training
example. The triple summation term gets added to the cost function after everything else is calculated, and it is just
the summation of all of the theta values (squared) of each layer.

- $ L $ is the number of layers
- $ S_l $ is the number of units in each layer

So the triple summation could be described as a triple nested for loop that loops through and sums all of the values in
a 3-dimensional array.

#### Getting Gradients For Minimizing the cost function

In order to get the gradients for the entire network, we first need to find the $ \delta $, which is
the partial derivative of the cost function with respect to z (the value of the layer before it goes through the
sigmoid) function.

$$
  \begin{aligned}
    \delta^{(l-1)}_j &= \frac{\partial}{\partial z^{(l-1)}} J(\theta) \\
    &= \frac{\partial J}{\partial z^{(l)}} \frac{\partial z^{(l)}}{\partial z^{(l-1)}} \\
    &= \delta^{(l)} \frac{\partial}{\partial z^{(l-1)}} z^{(l)} \\
    &= \delta^{(l)} \frac{\partial}{\partial z^{(l-1)}} (\Theta^{(l-1)} g(z^{(l-1)})) \\
    &= \delta^{(l)} \Theta^{(l-1)} .* g'(z^{(l-1)})
  \end{aligned}
$$

The $ \Delta $ term is a result of derviving the partial derivative of the cost function with respect
to $ \Theta $ which is the gradients that we are ultimately trying to find to plug into the functions
which iteratively minimize the cost function.

$$
  \begin{aligned}
  & \frac{\partial}{\partial\Theta^{(l)}} J(\Theta) = \Delta^{(l)} \\
  & \Delta^{(l)} = \delta^{(l+1)} * a^{(l)}
  \end{aligned}
$$

and then the gradient is ultimately scaled by the number of examples and lambda regularization is
added which makes the gradient equal to...

$$
\frac{\lambda}{m} \Delta^{(l)}
$$

The sizing of the matrices is an important design decision because they must all be compatible in order for forward
propogations and back propogation to work. For example, in forward propogation...

$$
  \begin{aligned}
    &examples = m \times 401 \\
    &\Theta^{(1)} = 25 \times 401 \\
    &\Theta^{(2)} = 10 \times 25 \\
    &a^{(2)} = examples * \Theta^{(1)T} = m \times 401 * 401 \times 25 = m \times 25 \\
    &a^{(3)} = a^{(2)} * \Theta^{(2)T} = m \times 25 * 25 \times 10 = m \times 10
  \end{aligned}
$$


at the end of forward prop we are left with `Mx10` one vs all predictions. Now it is time to go through the back
propogation cycle which should leave us with matrices of gradients that will match the sizes of
the $ \Theta $ matrices.

$$
  \begin{aligned}
    & \delta^{(3)} = m\times10 - m \times 10 = m\times10 \\
    & \delta^{(2)} = \Theta^{(2)T} * \delta^{(3)T} .* g'(z^{(2)})^{T} = 25 \times 10 * 10 \times m .* 25 \times m = 25 \times m \\
    & \Theta^{(1)}dx = \delta^{(2)} * examples / m = 25 \times m * m \times 401 = 25 \times 401 \\
    & \Theta^{(2)}dx = \delta^{(3)T} * a^{(2)} / m = 10 \times m * m \times 25 = 10 \times 25
  \end{aligned}
$$

so after running through the entire back propogation process we are left with theta gradients of `25x401` and `10x25`
which exactly match the size of the initial thetas which we can use for gradient descent or another algorithm to
minimize the cost function.
