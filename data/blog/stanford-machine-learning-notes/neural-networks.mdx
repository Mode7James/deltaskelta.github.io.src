---
title: Neural Networks
published: true
createdAt: 2019-02-26T14:20:26.310Z
updatedAt: 2019-03-04T04:20:03.243Z
categories:
  - Programming
  - Algorithms
  - Machine Learning
  - Neural Networks
---

Neural networks solve the problem of having too many quadratic combinations of features when designing a learning
algorithm. The features are fed into a "hidden" layer in the following way.

<BlockMath math="
  \begin{bmatrix}
    x_0 \\
    x_1 \\
    x_2
  \end{bmatrix}
  \to
  \begin{bmatrix}
  ...
  \end{bmatrix}
  \to
  h_\theta(x)
" />

inside the hidden layer is a matrix of theta values which looks like this

<BlockMath math="
  \begin{aligned}
  &\begin{bmatrix}
    x_0 \\
    x_1 \\
    x_2 \\
  \end{bmatrix}
  \to
  \begin{bmatrix}
    a_1^{(2)} \\
    a_2^{(2)} \\
    a_3^{(2)}
  \end{bmatrix}
  \to
  h_\theta(x) \\
  &\begin{bmatrix}
    x_0 \\
    x_1 \\
    x_2 \\
  \end{bmatrix}
  \to
  \begin{bmatrix}
    g(\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 + \theta_{12}^{(1)}x_2)) \\
    g(\theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 + \theta_{22}^{(1)}x_2)) \\
    g(\theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 + \theta_{32}^{(1)}x_2))
  \end{bmatrix}
  \to
  g(\theta_{10}^{(1)}a_0^{(2)} + \theta_{11}^{(2)}a_1^{(2)} + \theta_{12}^{(2)}a_2^{(2)} + \theta_{13}^{(2)}a_3^{(2)})
  \end{aligned}
" />

The jump from the second layer to the third layer where the hypothesis outputs a prediction is similar to the logistic
regression with no neural network. The difference in the neural network is that each input feature can be mapped to each
dimension of the hidden layer vector and the network can then combine the inputs according to some algorithm instead of
manually coming up with a function to fit intended shape of the data.

## The Example Problem

#### The cost function

The example problem for this week `ex3.m` first had us implement a vectorized cost function and gradient (partial
derivative). The arguments to this function are as follows.

1. The `X` is the entire training set, which is a `5000x400` of 5000 examples with 400 features each. Each pixel in the
handwritten digit is a feature.
2. It also takes thetas for each feature plus a bias, so 401 theta values
3. `y` is the expected output of the classification, which is a vector of `0`'s or `1`'s
4. lambda is the lambda regularization value

```matlab
function [J, grad] = lrCostFunction(theta, X, y, lambda)
```

#### The oneVsAll training

The second function that we had to implement was the training of the theta values for the on vs all comparisons.

```matlab
function [all_theta] = oneVsAll(X, y, num_labels, lambda)
```
1. `X` is the `5000x400` training set
2. `y` is the expected values of the digits. This is a `5000x1` matrix with the actual value of the 1-10 digit.
3. `num_labels` are the number of different labels or possible outcomes that we have.
4. `lambda` is the lambda regularization value that is to be fed to the cost function.
5. The return value `all_theta` is a `10x401` matrix. Each row of the matrix is used to identify one of the 10 labels
that we have.

to implement the function I made a loop that calls the `fmincg` function for every label that we have. The result is a
`10x401` matrix of theta values where each of the 10 rows is a set of theta values for every pixel that is trained to
identify whether an input is a single label (digit). If an input was to be tested, it would have to be tested against
all of the rows so that the question "is this a 1?" "is this a 2?" ... for every possible label is asked and answered.

the `y` input to the cost function is done in the form of `y == c` where `c` is the loop iterator which also
corresponds to the digit in the oneVsAll `y` arg. The argument that is passed to the cost function will be a `5000x1`
vector of `0`'s and `1`'s where the `1`'s are the oneVsAll digit we are trying to identify for this particular set of
thetas.

#### The prediction

for the prediction, we are not using a neural network and it is just a one vs all logistic regression. The function
takes two args which are

1. `all_theta` which is a `10x401` matrix where each row is a set of theta values to make a decision about a single
label.
2. `X` which is a `5000x400` matrix of 5000 training examples with 400 features each.
3. the return value `p` should be a `5000x1` vector of output hypotheses

```matlab
function p = predictOneVsAll(all_theta, X)
```

Multiplying the `all_theta` matrix with the `X'` matrix
gives <InlineMath math="[10 x 401] * [401 x 5000] = [10 x 5000]"/> so
in the end there will be a `10x5000` matrix where each column will be an output of something near 0 or near 1. If we
take the index of the max then we can construct the return value.

## Prediction With A Neural Network

the last problem in the set was to make a prediction using a neural network. This is done by multiplying the `5000x401`
input matrix by the `25x401` matrix (transposed) that makes up the first hidden layer. The output from this layer
is <InlineMath math="[5000 x 401] * [401 x 25] = [5000 x 25]" />. I like to visualize this as 25 different predictions
being made for every example input.

<BlockMath math="
  \begin{aligned}
    \begin{bmatrix}
      x_0 & ... & x_{401} \\
      & . & \\
      & . & \\
      & . &
    \end{bmatrix}
    *
    \begin{bmatrix}
      \theta_0 & ... & \theta_{401} \\
      & . & \\
      & . & \\
      & . &
    \end{bmatrix}
    =
    \begin{bmatrix}
     (x_0 * \theta_{0,0} + ... + x_{401} * \theta_{401,0}) & ... & (x_0 * \theta_{0,25} + ... + x_{401} * \theta_{401,25}) \\
     & . & \\
     & . & \\
     & . &
    \end{bmatrix}
  \end{aligned}
" />

Then the output from the second layer is ready to be fed into the third layer. The output from the second layer can be
seen as 5000 inputs with 25 features each since the first layer has cut the original 401 input features down to 25.
The <InlineMath math="l2" /> layer has 26 theta values, one for each of the outputs of <InlineMath math="l1" /> plus an
extra bias unit. This gives us the following multiplication of
matrices <InlineMath math="[5000 x 26] * [26 x 10] = [5000 x 10]" />.

<BlockMath math="
  \begin{aligned}
    \begin{bmatrix}
      a^{(2)}_{0,0} & ... & a^{(2)}_{0,25} \\
      & . & \\
      & . & \\
      & . &
    \end{bmatrix}
    *
    \begin{bmatrix}
      \theta^{(3)}_{0,0} & ... & \theta^{(3)}_{0,10} \\
      & . & \\
      & . & \\
      & . &
    \end{bmatrix}
    =
    \begin{bmatrix}
    (a^{(2)}_{0,0} * \theta^{(3)}_{0,0} + ... + a^{(2)}_{0,26} * \theta^{(3)}_{26,0}) & ... & (a^{(2)}_{0,0} *
    \theta^{(3)}_{0,10} + ... + a^{(2)}_{0,25} * \theta^{(3)}_{25,10}) \\
    & . & \\
    & . & \\
    & . &
    \end{bmatrix}
  \end{aligned}
" />

The output from this layer is the one vs all prediction, so each row of the `5000x10` output matrix is a row
of <InlineMath math="\approx 0 \quad or \quad \approx 1" />. If index 5 si approximately 1, then the prediction is that
the hadnwritten digit is a `5`.
