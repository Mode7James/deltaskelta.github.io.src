---
title: "Unsupervised Learning: Clustering"
published: true
createdAt: 2019-03-19T14:00:27.341Z
updatedAt: 2019-07-21T15:01:20.190Z
categories:
  - Programming
  - Algorithms
  - Machine Learning
  - Neural Networks
  - Stanford
---

## K-Means Algorithm

This algorithm starts with some points called cluster centroids, which will be the center point (average) of all of the
data in a given cluster. There are two steps to the algorithm that moves the position of the centroid based on the
average centerpoint of the points closest to it. The second step is to reclassify the points as belonging to the
centroid nearest to them. This will result in the centroids moving towards the center of a data cluster, which is
hopefully where we want to classify boundaries in the data.

```text
for i = 1 to m
  // "assign" the ith training example to the closest centroid
  c(i) := index(from 1 to K) of cluster centroid closest to x(i)

for k = 1 to K
  // move the centroid to the center of the points assigned to it
  u(k) := average of points assigned to cluster k
```

$$
J(C,M) = \frac{1}{m} \sum_{i=1}^m \begin{Vmatrix} x^{(i)} - \mu_{c^{(i)}} \end{Vmatrix}
$$

- the first loop of the algorithm is actually minimizing a cost function with respect to the `C` vector, which contains
  the index of the centroid that the ith training example belongs to.
- the second loop of the algorithm is minimizing the cost function with respect to the `Mu` vector which contains the
  average of the points assigned to the kth cluster

This algorithm can never have a cost function that increases like a logistic regression with a large alpha value would.

## Random Initialization

Since the centroids can arrive at local optimum points, when picking a random initial value, it is best to pick a point
right on top of a training example and also run the K-means algorithm a sufficient number of times to make sure that a
good initial randomization has been achieved. Out of the n times that the k-means algorithm was run, you can choose the
outcome that had the lowest cost as defined above. Iterating on the random initialization is more important as the
number of centroids becomes smaller relative to the dimensions of the dataset.

## Choosing The Proper Number of Clusters

Often the best way to go about choosing the right number of clusters it to look by hand for patterns in the data to see
different visualizations of the data.

#### The Elbow Method

This method of choosing the proper amount of clusters involves running the k-means algorithm multiple times with
increasing number of clusters and seeing where the "elbow" in the data is, which is the place where the decrease
in the cost with respect to increasing cluster amounts drastically decreases. This may work sometimes, but other
times, there could be an ambiguous decreasing curve that is hard to tell where to call the elbow.
