---
title: "Machine Learning: Hypothesis, Cost Function, Gradient Descent Notes"
published: true
createdAt: 2019-02-05T12:23:50.151Z
updatedAt: 2019-04-19T09:25:05.566Z
categories:
  - Programming
  - Algorithms
  - Machine Learning
---

import { M } from '../../../src/components/math';
import { Graph } from '../../../src/components/Charts/graph';

## Hypothesis

The hypothesis is the prediction about a set of data. It is the `y` value on a graph. In the house price example, the
x-axis is size in square feet and the y-axis is the price. The hypothesis would then be a linear function like
below. The `x` values are the values of each feature (of which there is only one -- house size).

<M b="h_\theta(x) = \theta_{0} + \theta_{1}x" />
<M b="f(x) = mx + b" />

It is similar to the normal linear function that everyone knows. These <M i="\theta" /> parameters are
chosen through an algorithm so that the cost function is minimized (the regression line) to accurately fit the data.

## The Cost Function

<M b="\frac{1}{2m} \displaystyle\sum_{i=1}^m (\check{y}_i - y_i)^2" />

since the y-axis (price) is the value we are trying to predict, we want to minimize the error of our prediction (fit the
best possible line to the data). We could do this by taking the difference of the predicted y value and the actual y
value when we plot the hypothesis for every point on the line. If we could find a line to minimize these differences,
that would be the line of best fit for the data.  <M i="\check{y}_i" /> in the cost function
formula is shorthand for <M i="h_{0}(x) \text{ or } \theta_{0} + \theta_{1}x" />

If we wanted to plot a possible hypothesis for the data we could plot something like below and then calculate the cost
function based on the error from the actual values of the data.

<Graph width="100%" points={[[1, 1], [2, 2], [3, 3]]} fx={(x) => 0.5 * x}/>

We can see that it missed the y values by `0.5, 1, 1.5` respectively. if we apply the cost function we would see that

<M b="\frac{1}{2 * 3} (.5^2 + 1^2 + 1.5^2) = \frac{1}{6} * 1.8125 = ~0.302" />

The cost function can be looked at in respect to <M i="\theta_{0} \text{ or } \theta_{1}" /> on a 2-dimensional
graph. The y-axis will be the cost function of <M i="\theta_{i}" /> which is annotated
with <M i="J(\theta_i)" /> and the x-axis will be <M i="\theta_{i}" />. In the chart above, the
hypothesis functions <M i="\theta_{1}" /> or slope is `0.5` and we can see that it does not fit the line very well.

If we went on to graph different lines of this function (the cost function with respect to <M i="\theta_1" />)
then we would quickly see that it is a quadratic function where the minimum is at <M i="\theta_1 = 1" /> or
where there is no error in the hypothesis (in this case because there is a perfect fit).

The graph would look something like the quadratic function of <M i="(x - 1)^2" /> which has its minimum at
`x = 1`

<Graph width="100%" fx={(x) => (x - 1) * (x - 1)} />

This was all assuming that the cost function was only a function of <M i="\theta_1" /> but in reality it is
a function of both <M i="\theta_1 \text{ and } \theta_0" /> so we must express it as such which is
actually a 3 dimensional function.

<M b="J(\theta_0, \theta_1) = \frac{1}{2m} \displaystyle\sum_{i=1}^m (\theta_0 + \theta_{1}x - y_i)^2" />

which means that we need a more sophisticated way to find a global minimum.

## Finding a Global Minimum Through Gradient Descent

The gradient descent function takes small steps towards the local minimum for every parameter (dimension) in the
hypothesis. since this example is a linear regression problem, there are only two parameters.

<M b="\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta_0,\theta_1)" />

the <M i="\alpha" /> is the size step that is taken for each step. The step is multiplied by the derivative
(which is the slope of the tangent line) to get a movement in the right direction. The right direction is guaranteed
because subtracting a positive derivative will push you in a downward direction and subtracting a negative derivative
(adding by a negative slope) will also push you in a downward direction.

This would be repeated for every paramter (dimension) of the hypothesis simultaneously before resetting the values to
the now lower values and repeating the process until minimums are found. A minimum is found when the derivative slowly
evens out to 0 meaning that the minimum is found and the next step is just <M i="\theta_j - 0" /> or static.
In reality it will often approach 0 but never actually get there.

## Deriving the cost function

This is taken from the answers to [this thread](https://math.stackexchange.com/questions/70728/partial-derivative-in-gradient-descent-for-two-variables)
which helped me understand everything after looking closely at the math.

#### Derivative of the hypothesis

since the hypothesis is <M i="h_{\theta}(x_{i}) = \theta_{0} + \theta_{1}x_{i}" />. We can derive the
following partial derivatives with respect to <M i="\theta_0 \text{and} \theta_1" />. The partial
derivative with respect to <M i="\theta_{0}" /> makes the term with <M i="\theta_1 x_1" />
irrelevant, so it changes to 0, and the partial derivative of a variable with no coefficient is 1.

<M b="
  \frac{\partial}{\partial\theta_{0}} h_{\theta}(x_{i})
  = \frac{\partial}{\partial\theta_{0}} (\theta_{0} + \theta_{1}x_{i})
  = \frac{\partial}{\partial\theta_{0}} \theta_{0} + \frac{\partial}{\partial\theta_{0}} \theta_{1}x_{i}
  = 1 + 0
  = 1
" />

When the derivative is taken with respect to <M i="\theta_{0}" /> then <M i="\theta_0" />
becomes irrelevant, and we need to take the derivative of the variable theta (which is 1), and leaves the `x` term
unchanged.

<M b="
  \frac{\partial}{\partial\theta_1} h_{\theta} (x_{i})
  = \frac{\partial}{\partial\theta_{1}} (\theta_{0} + \theta_{1}x_{i})
  = \frac{\partial}{\partial\theta_{1}} \theta_{0} + \frac{\partial}{\partial\theta_{1}}\theta_{1}x_{i}
  = 0 + x_{i}
  = x{i}
" />

Since the loss function is stated as above, we can get the derivative by doing the following...

<M b="
  \frac{\partial}{\partial\theta_j} J(\theta_0,\theta_1)
  = \frac{\partial}{\partial\theta_j} \Bigg[\frac{1}{2m} \sum_{i=1}^m (h_{\theta} (x_i) - y_i)^2 \Bigg]
" />

according to the [linearity of differentiation](https://en.wikipedia.org/wiki/Linearity_of_differentiation) we can
move the derivative into each of the summed terms. from the equation above

<M b="= \frac{1}{2m} \sum_{i=1}^m \frac{\partial}{\partial\theta_{j}} (h_\theta(x_i) - y_i)^2 "/>

and then according to the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) which states
that <M i="f(g(x))^\prime = f^\prime(g(x))g^\prime(x)" /> but the tricky part to see is that
the <M i="g(x)" /> is actually <M i="h_{\theta}(x_{i}) - y_i" />. If that is the case,
then the subsequent steps can be as follows.

<M b="
  \begin{aligned}
  &\frac{1}{2m} \sum_{i=1}^m 2(h_\theta (x_i) - y_i) \frac{\partial}{\partial \theta_j} (h_\theta (x_i) - y_i) \\
  =&\frac{1}{m} \sum_{i=1}^m (h_\theta (x_i) - y_i) \Bigg[ \frac{\partial}{\partial \theta_j} h_\theta (x_i) -
     \frac{\partial}{\partial \theta_j} y_i \Bigg] \\
  \end{aligned}
" />

at this point the derivative of <M i="y_i" /> drops off because its term has no <M i="\theta_j" />
involved and we are left with...

<M b="\frac{1}{m} \sum_{i=1}^m (h_\theta (x_i) - y_i) \frac{\partial}{\partial \theta_j} h_\theta (x_i)" />

and the derivative from the last term is known from above, so therefore

<M b="
  \begin{aligned}
    & \frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta (x_i) - y_i) \\
    & \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta (x_i) - y_i) x_i \\
  \end{aligned}
" />
