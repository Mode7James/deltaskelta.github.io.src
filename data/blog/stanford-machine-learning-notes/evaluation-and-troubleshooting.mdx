---
title: Evaluation and Troubleshooting
published: true
createdAt: 2019-03-14T01:56:11.402Z
updatedAt: 2019-06-05T01:50:06.194Z
categories:
  - Programming
  - Algorithms
  - Machine Learning
  - Neural Networks
  - Stanford
---

import { M } from '../../../src/components/math';

When troubleshooting the performance of a learning algorithm, it is best to try to narrow down the vause before jumping
to conclusions about what is causing it. Some common things that could be the problem are...

- get more training examples --> fixes high variance (overfitting)
- try fewer features --> fixes high variance (overfitting)
- try more features --> fixes high bias (underfitting)
- try adding polynomial features --> fixes high bias (underfitting)
- try changing <M i="\lambda" /> values. increasing --> fixes high variance (overfitting). decreasing --> fixes
  high bias (underfitting)

## Evaluating a Hypothesis

To evaluate a hypothesis, take part of the training data (~%70) and train the model with it, take the remaining (~30%)
and make predictions. If the training error is low and the test set error is high, then there is a problem somewhere. We
can begin to evaluate the hypothesis by calculating the error in the following way.

<M b="
  \begin{aligned}
    err(h\theta(x), y) &=
      \begin{cases}
        1 &\text{if } h_\theta(x) >= 0.5 \text{   and   } y=0 \text{   or if   } h_\theta(x) < 0.5 \text{   and   } y=1 \\
        0 &\text{otherwise}
      \end{cases} \\
    testErr &= \frac{1}{m_{test}} \sum_{i=1}^{m_{test}} err(h_\theta(x_{test}), y_{test})
  \end{aligned}
" />

## Model Selection

When selecting how to combine training data into a set of features, we need to avoid overfitting and find which degree
of polynomial features makes the best prediction for the training dat athat we have. One way to do this would be to
first split the training data into three seperate groups <M i="J_{\Theta}train \text{   } J_{\Theta}cv \text{  } J_{\Theta}test"/>.
`cv = cross validation`

We can then make different sets of polynomial features such as...

<M b="
  \begin{aligned}
    & 1. \quad \theta_0 + x_1\theta_1 + x_2\theta_2 + ... + x_n\theta_n \\
    & 2. \quad \theta_0 + x_1\theta_1 + x_1^2\theta_2 + ... + x_n^2\theta_{2n} \\
    & 3. \quad ...
  \end{aligned}
" />

We can then train each model and use the cross validation set to choose the one that gives the lowest error on the new
data. The test set is then used as a clean data set that has not been used that can verify the error is similar to the
level of error that we saw on the cross validation data set.

## Diagnosing Bias vs. Variance

After we have done the above, we can use the output of the training set and the cross validation set to see what is
happening.

- if <M i="J_{train}(\Theta)" /> is high and <M i="J_{cv}(\Theta)" /> is also high, there is
  probably an underfitting to the data and we should look to higher polynomial features. This is high bias.
- if <M i="J_{train}(\Theta)" /> is low and <M i="J_{cv}(\Theta)" /> is high then that means
  that the trained (low error) hypothesis did not perform well on new data and there is probably an overfitting problem.
  We should look to lower degree polynomial features. This is high variance.

## Choosing a Lambda Value

We still need to choose a lambda regularization value for each degree of polynomial. Testing lambda values can be done
by...

- A small lambda value means that there is a greater chance of overfitting the data.
- A high lambda value means that there is a greater chance of underfitting the data.

1. make a list of lambdas, starting small and doubling the value (like from 0.01 to 10.24).
2. create a list of models with different polynomials.
3. for each model, minimizing the cost function for each lambda value (nested for)
4. compute the cross validation (without lambda) error using the leanred networks (learned with lambda)
5. apply the best combo of the model + lambda on the clean test set and see if it still fits the data well.

## Learning Curves

If the learning algorithm has high bias (underfitting) , then getting more training data will probably not help. If we
think of a straight line fitting a polynomial curve, there is not much than can be done to make it fit better since it
just cannot bend in any way. There will be a small gap between the training error and the cross validation error because
they behave very similarly.

If the algorithm has high variance (overfitting), it is likely that getting more data will help because it will "level
out" the curve that is overfitting the data. There will be a larger gap between the training set and the cross
validation set though because the training hypothesis will bend to fit the examples exaclty, which doesn't model the
real world predictions all that well.

- These can be plotted by taking a small set of the training examples and seeing how the test error performs when adding
  more and more examples until the full set is utilized.
