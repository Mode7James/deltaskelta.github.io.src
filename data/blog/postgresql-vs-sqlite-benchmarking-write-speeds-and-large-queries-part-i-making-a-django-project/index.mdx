---
title: "PostgreSQL vs. SQLite: Benchmarking Write Speeds and  Large Queries (Part I: Making a Django Project)"
published: true
createdAt: 2017-01-05T22:38:42.000Z
updatedAt: 2019-04-19T04:53:35.527Z
images:
  - ./Screen_Shot_2016-12-22_at_2.33.47_PM.png
categories:
  - Programming
  - Python
---

I had trouble with a database query taking up a critical amount memory on a server than I run. At ~~deprecated site~~
users can select how many points to plot on a map out of a database of ~70,000 data points. The database happens to
be SQLite and I was wondering if I could squeeze any performance from just switching databases rather than upgrading
the box or refactoring the code.

I wanted to see if switching to PostgreSQL would lighten the load on memory. I started the site on SQLite since there
would only be occasional writes by one user. according to the [SQLite](https://sqlite.org/whentouse.html) website,
regarding when to use SQLite, my application fit perfectly within their guidelines. It would be well less than a TB
of disk space and it would only have one user writing occasionally and a modest amount of readers.

I am sure my method is far from perfect but I wanted to get a feel for how the two different databases performed,
nonetheless.

## Creating A Django Project to Work With

<props.imgs.Img1 width="20%" align="left" />

I created a Django project that has a simple *REST API* for creating, retrieving and destroying data. I made the
model structure reasonable close to what I used in production and my goal is to get the total time elapsed for
requests as well as memory usage information from graphite.

I have a server that I had setup with [graphite](https://github.com/graphite-project/graphite-web) which I cloned,
and removed all that was unnecessary. I was left with the smallest Digital Ocean droplet with these specs.

I set up Django to serve directly through gunicorn without going through NGINX or any other reverse proxy server. So
once I verified that the Django project and graphite were running and collecting stats from `collectd` and `statsd`,
I was ready to go.

I made a separate app for *PostgreSQL* and *SQLite* but they both use basically the same model structure which is
posted below. It has a method to populate three `CharField`s of length 50 with random letters and numbers.

```python
from __future__ import unicode_literals
import random
import string
from django.db import models

# Create your models here.
class SqliteData(models.Model):
    """A model with random strings to simulate sqlite data"""

    field_1 = models.CharField(max_length=50)
    field_2 = models.CharField(max_length=50)
    field_3 = models.CharField(max_length=50)

    def populate_data(self):
        """Fills the model with random strings as data"""
        random_string = ''.join(random.choice(string.ascii_uppercase + string.ascii_lowercase + string.digits) for _ in range(150))
        field_1, field_2, field_3 = random_string[0:50], random_string[50:100], random_string[100:]
        self.save()

```

The views are fairly straightforward, they time how long it takes to process the data and return a plain
`HttpResponse` with the elapsed time in the view code block.

```python
def create_data(request, n):
  """Renders a template to run the benchmark test on psql"""
  start = timer()
  for i in range(int(n)):
    obj = PsqlData()
    obj.populate_data()
  end	= timer()
  return HttpResponse(end - start)
```

The retrieve data view serializes the queryset into JSON and then returns that as a JSON object as well. I did this
to force the query to be evaluated since Django
[querysets are lazy](https://docs.djangoproject.com/en/1.10/topics/db/queries/#querysets-are-lazy).

```python
def retrieve_data(request):
  """Retrieve all of the objects that are in the database"""
  start = timer()
  objs = PsqlData.objects.using('postgresql').all()
  data = {}
  for i, obj in enumerate(objs):
    data[i] = [obj.field_1, obj.field_2, obj.field_3]
  json = JsonResponse(data)
  end = timer()
  return HttpResponse('%s<script>var data = %s</script>' % (end - start, json.content))
```

The destroy view destroys all the objects in the database and times how long that operation takes.

```python
def destroy_data(request):
  """Destroy all the data objects in the database"""
  start = timer()
  objs = PsqlData.objects.using('postgresql').all()
  for obj in objs:
    obj.delete()
  end = timer()
  return HttpResponse(end - start)
```

After testing to make sure that everything works as I expected it to, I just needed to write a python script to
actually run the test on the server and log the results.

This logs the start time from the `datetime` module and also logs the finish time when writing the log. I also get
the time elapsed in the view (which is sent from the Django view). So once I crunch the data I should be able to see
the amount of time that is lost in the network and actually returning the values in the from the view, which will
probably be significant in the retrieve view.

```python
import time
import requests
from datetime import datetime

api_server = 'http://my.server.ip.addy:8500'
psql = '/psql'
sqlite = '/sqlite'
create = '/create'
retrieve = '/retrieve'
destroy = '/destroy'

def hit_api(server, db, operation, n=0):
  """request API endpoint to create n objects"""
  # Getting the time right before the request is initiated. Time is meant for a reference when
  # looking at the graphite graphs of the server
  time = datetime.now()
  if n == 0:
    # Changing n to 'all' so I can reference it when writing the output. Retrieve and Destroy
    # work on 'all' data in the database
    n = 'all'
    response = requests.get('%s%s%s' % (server, db, operation))
  else:
    response = requests.get('%s%s%s/%s' % (server, db, operation, n))

  with open('/Users/Jeff/Development/benchmark-%s.txt' % db[1:], 'a+') as f:
    # logging the operation...on...start_time...elapsed time returned by response
    f.write('%s: %s -- started: %s - finished: %s -- %s\n' \
        % (operation[1:], n, time, datetime.now(), response.content.split("<script>")[0]))

def main():
  """The main function which will run and log all the operations"""
  amounts = [500, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000]
  for num in amounts:
    # Doing all operations for num in sqlite
    hit_api(api_server, sqlite, create, num)
    hit_api(api_server, sqlite, retrieve)
    hit_api(api_server, sqlite, destroy)
    time.sleep(60)
    # Doing all operations for num in psql
    hit_api(api_server, psql, create, num)
    hit_api(api_server, psql, retrieve)
    hit_api(api_server, psql, destroy)
    time.sleep(60)

if __name__ == "__main__":
  main()
```
